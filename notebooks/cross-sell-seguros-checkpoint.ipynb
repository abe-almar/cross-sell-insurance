{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amended-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#clientes=pd.read_csv(\"clientes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes[\"Vehicle_Age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes[\"Response\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup=clientes.duplicated()\n",
    "duplicates=dup.sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-hawaiian",
   "metadata": {},
   "source": [
    "### ** Por qué hay clientes sin carné? **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample=clientes.sample(n=30000)\n",
    "\n",
    "sample.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marked-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_si=sample[sample[\"Response\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_si.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_si.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_si.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-charleston",
   "metadata": {},
   "source": [
    "### SI COGEMOS SOLO LOS QUE HAN DICHO QUE SI, VEMOS QUE LA EDAD SIGUE UNA DISTRIBUCIÓN NORMAL CENTRADA EN LOS 45 AÑOS, LA REGIÓN 28 DESTACA, TODOS SON CLIENTES NUEVOS Y HAY 2 CANALES DE COMUNICACIÓN SUPERIORES A LOS DEMÁS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-organ",
   "metadata": {},
   "source": [
    "## COMO QUEREMOS QUE SEAN CLIENTES YA EXISTENTES, PODRÍAMOS OBVIAR EL CAMPO \"PREVIOUSLY_INSURED\" Y VER QUÉ OTRAS CARACTERÍSTICAS HACEN QUE HAYA INTERÉS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-harvard",
   "metadata": {},
   "source": [
    "# CREAMOS EL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set=train_test_split(clientes, test_size=0.2, random_state=42) #Utilizamos el método de sklearn para hacer el random split del dataset\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes=train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "going-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos nuestro dataset de etiquetas y el dataset de parámetros\n",
    "clientes_labels=clientes['Response'].copy()\n",
    "clientes = clientes.drop(columns=['Response', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-adams",
   "metadata": {},
   "source": [
    "### Tenemos que hacer OneHotEncoding en todas aquellas columnas que se corresponden a categorías y que no tengan 1-0, Gender, Driving_Licence, RegionCode, Vehicle_Age, Vehicle_Damage, Policy_Sales_Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ranging-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a transformar\n",
    "#columns_to_encode = ['Gender', 'Region_Code', 'Driving_License', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas seleccionadas\n",
    "#label_encoders = {}\n",
    "#for col in columns_to_encode:\n",
    "#    encoder = LabelEncoder()\n",
    "#    clientes[col] = encoder.fit_transform(clientes[col])\n",
    "#    label_encoders[col] = encoder  # Guardar el LabelEncoder para cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thirty-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Aplicar OneHotEncoder solo a las columnas que han sido LabelEncoded\n",
    "#encoded_columns = onehot_encoder.fit_transform(clientes[columns_to_encode])\n",
    "\n",
    "# Crear un DataFrame con las columnas codificadas\n",
    "#clientes_cat_1hot = pd.DataFrame(encoded_columns, columns=onehot_encoder.get_feature_names_out(columns_to_encode))\n",
    "\n",
    "# Verifica que las dimensiones sean correctas\n",
    "#print(f\"Shape of original dataframe: {clientes.shape}\")\n",
    "#print(f\"Shape of one-hot encoded dataframe: {clientes_cat_1hot.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alpine-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "defined-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador personalizado para transformar las variables numéricas que deberían ser objects, para su posterior encoding\n",
    "class ChangeDtypeToObject(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns  # Columnas que deseas convertir a object\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No necesitamos hacer nada en el fit para este caso\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Copiar el DataFrame para evitar cambios no deseados en los datos originales\n",
    "        X_transformed = X.copy()\n",
    "        # Cambiar el tipo de dato de las columnas especificadas a 'object'\n",
    "        for col in self.columns:\n",
    "            X_transformed[col] = X_transformed[col].astype('object')\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "third-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador personalizado para eliminar variables colineales porque hemos visto que aparecen demasiadas variables al final del pipeline\n",
    "class RemoveCollinearFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.9):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convertir a denso si es un array disperso\n",
    "        if isinstance(X, csr_matrix):\n",
    "            X = X.toarray()\n",
    "        corr_matrix = pd.DataFrame(X).corr().abs()  # Matriz de correlación\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        self.to_drop = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, csr_matrix):\n",
    "            X = X.toarray()\n",
    "        return pd.DataFrame(X).drop(columns=self.to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "devoted-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los pipelines de las transformaciones por separado\n",
    "\n",
    "# Especificar las columnas\n",
    "columns_to_convert = ['Region_Code', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']  # Columnas categóricas no binarias\n",
    "numeric_columns = ['Age', 'Annual_Premium']        # Columnas numéricas\n",
    "binary_columns = ['Previously_Insured', 'Driving_License']  # Columnas binarias\n",
    "categorical_columns = ['Gender', 'Region_Code', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']  # Columnas categóricas no binarias (ya excluí 'Driving_License')\n",
    "\n",
    "# Pipeline para procesar atributos numéricos\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputar valores faltantes\n",
    "    ('scaler', StandardScaler())  # Estandarizar las variables numéricas\n",
    "])\n",
    "\n",
    "# Pipeline para procesar atributos categóricos no binarios\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputar valores faltantes\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))  # Codificar las variables categóricas\n",
    "])\n",
    "\n",
    "# Pipeline para procesar atributos categóricos binarios (sin transformación)\n",
    "binary_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))  # Imputar valores faltantes\n",
    "])\n",
    "\n",
    "# Pipeline para seleccionar y procesar las columnas numéricas, categóricas y binarias\n",
    "processing_pipeline = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_pipeline, numeric_columns),        # Procesar columnas numéricas\n",
    "    ('cat', categorical_pipeline, categorical_columns),# Procesar columnas categóricas no binarias\n",
    "    ('bin', binary_pipeline, binary_columns)           # Procesar columnas categóricas binarias\n",
    "])\n",
    "\n",
    "# Pipeline final con PCA y eliminación de colinealidad\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('feature_processing', processing_pipeline),               # Procesar atributos numéricos, categóricos y binarios\n",
    "    ('remove_collinear', RemoveCollinearFeatures(threshold=0.9)),  # Eliminar colineales\n",
    "    ('pca', PCA(n_components=0.95))                            # Aplicar PCA para mantener el 95% de la varianza\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "devoted-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el pipeline a los datos\n",
    "clientes_transformed = final_pipeline.fit_transform(clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306544a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-edition",
   "metadata": {},
   "source": [
    "# PROBAMOS ALGUNOS MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-telephone",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(clientes_transformed, clientes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = log_reg.predict(clientes_transformed)\n",
    "accuracy = accuracy_score(clientes_labels, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(clientes_labels, y_pred)\n",
    "recall = recall_score(clientes_labels, y_pred)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1=f1_score(clientes_labels, y_pred)\n",
    "\n",
    "print(f'F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-intranet",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model=DecisionTreeClassifier(max_depth=5)\n",
    "tree_model.fit(clientes_transformed, clientes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree_model.predict(clientes_transformed)\n",
    "accuracy_tree = accuracy_score(clientes_labels, y_pred_tree)\n",
    "print(f'Accuracy: {accuracy_tree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_tree = precision_score(clientes_labels, y_pred_tree)\n",
    "recall_tree = recall_score(clientes_labels, y_pred_tree)\n",
    "print(f'Precision: {precision_tree}, Recall: {recall_tree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores_log= cross_val_score(log_reg, clientes_transformed, clientes_labels, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f'Mean AUC-ROC: {np.mean(cv_scores_log)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_tree= cross_val_score(tree_model, clientes_transformed, clientes_labels, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f'Mean AUC-ROC: {np.mean(cv_scores_tree)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-elements",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "greater-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model=RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#rf_model.fit(clientes_transformed, clientes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the Random Forest model\n",
    "cv_scores = cross_val_score(rf_model, clientes_transformed, clientes_labels, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f'Cross-validated AUC-ROC scores: {cv_scores}')\n",
    "print(f'Mean AUC-ROC: {cv_scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight':['balanced']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring='roc_auc', \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(clientes_transformed, clientes_labels)\n",
    "\n",
    "# Output the best parameters and best score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best AUC-ROC Score: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-protocol",
   "metadata": {},
   "source": [
    "### The Random Forest model shows strong accuracy and precision, but the recall and AUC-ROC scores suggest there may be room for improvement in correctly identifying all positive cases (customers likely to subscribe to additional insurance). The relatively high precision but lower recall means that the model is good at predicting true positives, but it is missing some potential customers who would have responded positively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-logan",
   "metadata": {},
   "source": [
    "Next Steps: improve Recall and AUC-ROC . HOW: \n",
    "- Lower the decision threshold to increase recall (at the expense of some precision), meaning more customers are classified as likely to buy.\n",
    "- Handle class imbalance. There are significantly more negatives than positives in the dataset so we could set the class_weights of RF to \"balanced\" to penalize the model more for misclassifying minority classes (positives in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf_model = RandomForestClassifier(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "    random_state=42,\n",
    "    class_weight='balanced')\n",
    "\n",
    "final_rf_model.fit(clientes_transformed, clientes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower the decision threshold to find the perfect balance between precison and recall\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_prob_final = final_rf_model.predict_proba(clientes_transformed)[:, 1]\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precisions_balanced, recalls_balanced, thresholds_balanced = precision_recall_curve(clientes_labels, y_prob_final)\n",
    "\n",
    "# Find the threshold that balances precision and recall\n",
    "f1_scores = 2 * (precisions_balanced * recalls_balanced) / (precisions_balanced + recalls_balanced)\n",
    "best_threshold_balanced = thresholds_balanced[np.argmax(f1_scores)]\n",
    "print(f'Best Threshold: {best_threshold_balanced}')\n",
    "\n",
    "# Use the new threshold for predictions\n",
    "y_pred_rf_balanced = (y_prob_final >= best_threshold_balanced).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(clientes_labels, y_pred_rf_balanced)\n",
    "recall = recall_score(clientes_labels, y_pred_rf_balanced)\n",
    "print(f'Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_rf_balanced= cross_val_score(final_rf_model, clientes_transformed, clientes_labels, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f'Mean AUC-ROC: {np.mean(cv_scores_rf_balanced)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-candidate",
   "metadata": {},
   "source": [
    "# A pesar del desequilibrio del dataset tenemos unos buenos resultados en training. Pasamos a test con el modelo que hemos entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "specified-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= test_set.drop(columns=['Response', 'id'])\n",
    "y_test=test_set[\"Response\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prepared=final_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "broad-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions=final_rf_model.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = best_threshold_balanced\n",
    "\n",
    "predicted_proba = final_rf_model.predict_proba(X_test_prepared)\n",
    "predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "precision_test = precision_score(y_test, predicted)\n",
    "recall_test = recall_score(y_test, predicted)\n",
    "print(f'Precision: {precision_test}, Recall: {recall_test}')\n",
    "\n",
    "cv_scores_test= cross_val_score(final_rf_model, X_test_prepared, y_test, cv=5, scoring='roc_auc')\n",
    "print(f'Mean AUC-ROC: {np.mean(cv_scores_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after transformation\n",
    "# OneHotEncoder creates new columns, so we need to capture the transformed feature names\n",
    "num_features = numeric_columns\n",
    "cat_features = final_pipeline.named_steps['feature_processing'].transformers_[1][1]['onehot'].get_feature_names_out(categorical_columns)\n",
    "bin_features = binary_columns\n",
    "all_features = list(num_features) + list(cat_features) + list(bin_features)\n",
    "# Since PCA and collinearity removal reduce the features, we can't directly map them after those steps.\n",
    "# Therefore, we will use the pipeline right before PCA and collinearity removal to understand the feature importance.\n",
    "\n",
    "# Refit the pipeline up to feature processing to get transformed data before PCA and collinearity removal\n",
    "pre_pca_data = final_pipeline.named_steps['feature_processing'].transform(clientes)\n",
    "\n",
    "# Re-train RandomForest on pre-PCA, pre-collinearity data for feature importance extraction\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_depth=grid_search.best_params_['max_depth'],\n",
    "    min_samples_split=grid_search.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "    random_state=42,\n",
    "    class_weight='balanced')\n",
    "rf.fit(pre_pca_data, clientes_labels)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a sorted list of features with their importance values\n",
    "feature_importance = sorted(zip(all_features, importances), key=lambda x: x[1], reverse=True)\n",
    "# Keep only top 5 of features\n",
    "feature_importance = feature_importance[:5]\n",
    "# Display feature importance\n",
    "for feature, importance in feature_importance:\n",
    "    print(f'Feature: {feature}, Importance: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-hearts",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
